## 机器学习的评估指标

### 基本的评价指标

#### 准确率 

分类正确的个例 / 总样本个数

- 准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素

#### 精确率与召回率  P-R 曲线

混淆矩阵 由正例、反例和预测的正例反例组成

<table border="1">
  <tr>
    <th>预测\真实</th>
    <th>正例</th>
    <th>反例</th>
  </tr>
  <tr>
    <td>正例</td>
    <td>TP</td>
    <td>FP</td>
  </tr>
  <tr>
    <td>反例</td>
    <td>FN</td>
    <td>TN</td>
  </tr>
</table>

精确率(precision) = TP / TP + FP   模型预测为正的样本中实际情况也为正的概率  
召回率(recall)  = TP / TP + FN  实际为正的样本中模型预测为正的概率

精确率和召回率这两个指标各有侧重。精确率和召回率是此消彼长的关系

精确率越大，召回率越小；精确率越小，召回率越大。

比如模型完全可以把所有样本预测为正这样召回率是百分百， 比如只预测一个样例为正这样精确率就是百分之百

#### F1 score
F1 score , 综合考虑了精确率和召回率这两个指标

F1 score是精确率和召回率的调和平均，其值越大，模型的效果越好。

#### AUC ROC 曲线

TPR 真正例率 = TP / TP + FN  实际为正的样本中模型预测为正的概率

FPR 假正例率 = FP / FP + TN  有多少负例被判断成正例

ROC 曲线以 FPR 和 TPR 为横纵坐标 选取不同的阈值区分正负例，计算横纵坐标画出坐标点，连线形成
AUC 就是 ROC 曲线与坐标轴形成的区域的面积 



#### 正负样本及其不均衡的情况下采用 F1 和 AUC 评估



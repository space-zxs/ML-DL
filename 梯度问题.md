## 梯度消失和梯度爆炸

### 梯度消失的原因  和  激活函数的优缺点分析

对神经网络而言，它是个多层结构，而且理论上来说是可以达到任意多层的，因为在反向传播过程中，求权重的偏导数时，是用到了链式求导，如果其中有小于1的乘数比如sigmoid\tanh的导数
，那么随着层数的增加，梯度会越来越小,可能会出现梯度消失
 
### 激活函数的发展

激活函数的发展经历了Sigmoid -> Tanh -> ReLU -> Leaky ReLU -> Maxout这样的过程，还有一个特殊的激活函数Softmax，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化

### 激活函数的对比

#### 饱和 
是指当取值趋于无穷函数导数趋于零

sigmoid 的函数是   1 / 1 + exp(-z)  函数恒大于0 不关于原点对称， 函数的倒数的取值在[0, 0.25] 之间， 导数小于1

#### sigmodi的缺点
- Sigmoid函数饱和使梯度消失。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。
- Sigmoid函数的输出不是零中心的。因为如果输入神经元的数据总是正数，那么关于的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。

tanh(x) = 2*sigmoid(2x) - 1  对于tanh来说 改进了 sigmoid 不对于 0 对称的问题，函数导数的范围在[0，1] 之间 同样可能造成梯度消失
#### tanh 的问题
- Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。
= 为了防止饱和，现在主流的做法会在激活函数前多做一步batch normalization，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。

 ![image](https://user-images.githubusercontent.com/77714764/233403468-b08011bf-e4b6-4f05-8f50-87979f435236.png)

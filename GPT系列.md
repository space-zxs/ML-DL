### GPT论文 ###

#### GPT ####
 ##### 摘要 #####
  使用的没有标号的数据，然后在根据子任务做微调。
  对比之前的工作不需要改变模型的结构只需要改变输入的形式适合模型输入
  基于多层transformer的解码器
  无监督预训练
  监督微调： 输入一个序列和标号到训练好的模型得到输出概率
  任务： 分类任务， 蕴含任务（三分类），相似性任务，
  ![image](https://github.com/space-zxs/ML-DL/assets/77714764/3b5bc559-0402-4837-b1c6-e3abb6755e71)
 
 bert使用的数据大概是gpt的四倍训练了一个三倍gpt大小的模型，3.4亿参数，
 
 #### GPT2 #####
 
 ###### zero shot ######
 子任务不在需要另外训练模型
 
#### GPT3 ####

few-shot 子任务上给一些

GPT-3 一个自回归模型拥有175亿参数

子任务：
 需要一个子任务数据集

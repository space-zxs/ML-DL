## NLP基础串连
_从RNN -> LSTM -> Attention家族 -> Transformer -> LLM系列_

---
### RNN 

_**提出的前提**_

- 传统文本处理任务的方法中一般将**TF-IDF**向量作为特征输入。显而易见，这样的表示实际上丢失了输入的文本序列中每个单词的顺序
- **卷积神经网络**处理文本信息时，通过**滑动窗口加池化的方式**将原先的输入转换成一个固定长度的向量表示，这样做可以捕捉到原文本中的一些**局部特征**，但是两个单词之间的**长距离依赖关系**还是很难被学习到。

_**RNN能很好的解决上面的问题，顺序和长距离依赖**_

                                  经典RNN结构

![经典RNN结构](https://github.com/space-zxs/ML-DL/assets/77714764/0b05a54c-d670-492d-a0cd-855b4542f133)

                                  
第t层的隐含状态ht编码了序列中前t个输入的信息，可以通过当前的输入xt和上一层神经网络的状态ht−1计算得到；最后一层的状态hT编码了整个序列的信息

例如，在hT后面直接接一个Softmax层，输出文本所属类别的预测概率y，就可以实现文本分类

![image](https://github.com/space-zxs/ML-DL/assets/77714764/91730ba8-a90f-4c9d-b262-345ad6156627)

其中f和g为激活函数，U为输入层到隐含层的权重矩阵，W为隐含层从上一时刻到下一时刻状态转移的权重矩阵。在文本分类任务中，f可以选取Tanh函数或者ReLU函数，g可以采用Softmax函数。

### RNN中的梯度问题

_**RNN中为什么会出现梯度消失或梯度爆炸？又该如何解决？**_

根本原因在于[反向传播算法](https://www.cnblogs.com/charlotte77/p/5629865.html) 链式求导法则

并且存在激活函数，当激活函数的导数大于1的时候，梯度大小会呈指数增长，导致梯度爆炸，反之，梯度的大小会呈指数缩小，产生梯度消失

[梯度爆炸的解决办法](),[梯度消失的解决办法]()

_**更换RELU函数还是不能解决梯度问题的原因**_
![image](https://github.com/space-zxs/ML-DL/assets/77714764/fc8aef27-4cf6-4710-9e3e-2058a4f13938)

因为w的n次方在单位矩阵周围趋近于1

---

### LSTM

- RNN中的**梯度消失**和**梯度爆炸**的问题，学习能力有限，在实际任务中的效果往往达不到预期效果
- LSTM可以对有价值的信息进行**长期记忆**，从而减小RNN的学习难度

_**LSTM是如何实现长短期记忆功能的？**_

                                LSTM结构图
                                
  ![image](https://github.com/space-zxs/ML-DL/assets/77714764/e10a46f3-ea94-4de3-abe2-53507e16d9d0)

与RNN相比，LSTM**仍然是基于xt和ht−1来计算ht**，只不过对内部的结构进行了更加精心的设计，加入了**输入门it、遗忘门ft以及输出门ot三个门和一个内部记忆单元ct**。输入门控制当前计算的新状态以多大程度更新到记忆单元中；遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉；输出门控制当前的输出有多大程度上取决于当前的记忆单元。

经典的LSTM中，第t步的更新计算公式为：

![image](https://github.com/space-zxs/ML-DL/assets/77714764/108bce27-b08c-46fb-948a-537c6b930188)

输入门it的结果是向量，其中每个元素是0到1之间的实数，用于控制各维度流过阀门的信息量；Wi、Ui两个矩阵和向量bi为输入门的参数，是在训练过程中需要学习得到的。遗忘门ft和输出门ot的计算方式与输入门类似，它们有各自的
参数W、U和b。与传统的循环神经网络不同的是，从上一个记忆单元的状态ct−1到当前的状态ct的转移不一定完全取决于激活函数计算得到的状态，还由输入门和遗忘门来共同控制

_**LSTM里各模块分别使用什么激活函数，可以使用别的激活函数吗？**_


_在LSTM中，遗忘门、输入门和输出门使用Sigmoid函数作为激活函数；在生成候选记忆时，使用双曲正切函数Tanh作为激活函数。值得注意的是，这两个激活函数都是饱和的，也就是说在输入达到一定值的情况下，输出就不会发生明显变化了。如果是用非饱和的激活函数，例如ReLU，那么将难以实现门控的效果。 Sigmoid函数的输出在0～1之间，符合门控的物理定义。且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关。在生成候选记忆时，使用Tanh函数，是因为其输出在−1～1之间，这与大多数场景下特征分布是0中心的吻合。此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。_

### seq2seq 

- Seq2Seq模型提出之前，深度神经网络在图像分类等问题上取得了非常好的效果。在深度学习擅长的问题中，**输入和输出通常都可以表示为固定长度的向量**，如果长度稍有变化，会使用补零等操作

Seq2Seq模型的核心思想是，通过深度神经网络将一个作为输入的序列映射为一个作为输出的序列，这一过程由编码输入与解码输出两个环节构成。

经典实现使用

---
#### Attention is all your need
##### 论文
### ###

### 摘要部分 ###
  1、说明了transformer的架构，没有使用之前的rnn或者cnn模块二十提出了一种纯基于注意力的新模型架构
  2、论文的实验是基于两个机器翻译任务，从德语到英语，取得了不错的分数
 
### 结论部分 ### 
    一个完全基于注意力的序列模型，使用多头注意力替换了传统的基于rnn的架构
### 模型结构图 ###
![image](https://github.com/space-zxs/ML-DL/assets/77714764/d8849731-4b63-4f93-9761-c5e917f0222f)



###
### 介绍 ###

rnn的对比和缺点，不能并行容易梯度消失，忘记开始学的信息

编码器和解码器

对于输入（x1，x2，x3.....） 经过编码器得到(z1,z2,z3....zn)，每一个x代表一个词或一个句子，每一个z代表他的表示向量.

把z传入解码器，得到输出序列（y1，y2，y3.。。。）, 对于每一步来说都是一个自回归的过程，用先前生成的字符作为一个加和作为下一个输出的输入。

#### model ####
  一个transformer解码器块有6层，每一层有两个子层，一个多头注意力层一个全连接层，输出的维度是512，总体来说只有两个参数可以调，块数n和输出维度512
  一个transformer编码器块有6层，但是插入了一个第三层mask层，保证预测的时候不会看到后面的句子
  ##### bn 和 ln的对比
    bn是对batch的每个特征做n，而ln是对每个样本做n
    
    对于transformer是一个三维的输出， 批量，序列，特征 
    
    为什么ln的效果好一点？
    
    对于序列数据来说，每个样本的长度不一定统一的
    
  ##### attention
    
    
  并行计算通过矩阵乘法实现
  
  mask 实现只会看前t时刻的qkv  通过把后面的k置为一个很大的负数这样softmax出来就接近0
  
  
  muti-head
  
  先把qkv 投影到低维 在做注意力，在投影回来，
  ##### position edcoding 
    sin 和 cos 加入时间信息，改变qkv的顺序不影响最终输出
    
    
  #### why self-attention 


### Transformer 面试问题汇总

1. Transformer 结构分拆
   ![image](https://github.com/space-zxs/ML-DL/assets/77714764/0464cbf2-15df-4253-aa08-6b52ff25849a)


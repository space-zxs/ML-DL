#### Attention is all your need
##### 论文
### ###

### 摘要部分 ###
  1、说明了transformer的架构，没有使用之前的rnn或者cnn模块二十提出了一种纯基于注意力的新模型架构
  2、论文的实验是基于两个机器翻译任务，从德语到英语，取得了不错的分数
 
### 结论部分 ### 
    一个完全基于注意力的序列模型，使用多头注意力替换了传统的基于rnn的架构
  
### 介绍 ###

rnn的对比和缺点，不能并行容易梯度消失，忘记开始学的信息

编码器和解码器

对于输入（x1，x2，x3.....） 经过编码器得到(z1,z2,z3....zn)，每一个x代表一个词或一个句子，每一个z代表他的表示向量.

把z传入解码器，得到输出序列（y1，y2，y3.。。。）, 对于每一步来说都是一个自回归的过程，用先前生成的字符作为一个加和作为下一个输出的输入。

#### model ####
  一个transformer解码器块有6层，每一层有两个子层，一个多头注意力层一个全连接层，输出的维度是512，总体来说只有两个参数可以调，块数n和输出维度512
  一个transformer编码器块有6层，但是插入了一个第三层mask层，保证预测的时候不会看到后面的句子
  ##### bn 和 ln的对比
    bn是对batch的每个特征做n，而ln是对每个样本做n
    
    对于transformer是一个三维的输出， 批量，序列，特征 
    
    为什么ln的效果好一点？
    
    对于序列数据来说，每个样本的长度不一定统一的
    
  ##### attention
    
    
  并行计算通过矩阵乘法实现
  
  mask 实现只会看前t时刻的qkv  通过把后面的k置为一个很大的负数这样softmax出来就接近0
  
  
  muti-head
  
  先把qkv 投影到低维 在做注意力，在投影回来，
  ##### position edcoding 
    sin 和 cos 加入时间信息，改变qkv的顺序不影响最终输出
    
    
  #### why self-attention 

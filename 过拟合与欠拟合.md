### 过拟合的解决办法

正则化  增加数据量  early stop  dropout batcnormalization

#### dropout 能防止过拟合的原因

类似去平均值的左右，去平均值，可以一定程度上降低过拟合 而 dropout随机丢弃一些神经元，其实就类似于取平均值

减少了神经元之间复杂的共适应关系，因为dropout的随机性导致神经元并不是固定出现的，这样多变的结构使得模型更容易学到鲁棒的特征

### batcnormalization能防止过拟合的原因

因为其作用与批量的样本上，输出受整个批量的影响，相当于做了数据增强，受单一样本的影响比较小。

优点：
加快训练和收敛速度 

每层输入的数据分布差别很大的话会不利于训练和收敛，通过将每层的数据减去均值比上方差将数据缩放，改变数据分布，使得数据分布的变化减少。
